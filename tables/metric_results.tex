\begin{table}[h]
	\centering
	\caption{Results.}
	\begin{tabular}{l|c|c|c}
		Topic Model & Perplexity & \makecell{Topic \\ Coherence} & \makecell{Topic \\ Difference} \\
		\midrule
		\Acrlong{lda} & 6761.6 & 0.520 & 0.575 \\
		Author-Topic Model & 10991.5 & 0.335 & 0.615 \\
		Category-Topic Model & 11126.0 & 0.370 & 0.560 \\
		Taxonomy-Topic Model & - & 0.660 & 0.709 \\
	\end{tabular}
	\label{tab:metric_results}
\end{table}

From \autoref{tab:metric_results}, we can see that the Author and Category-Topic models are performing the worst, whereas the Taxonomy-Topic model is outperforming all other models.
However, the run time of the Taxonomy model is worse than the standard \gls{lda}.
It takes about 6-8 hours to compute 50 epochs for the \gls{lda} model, depending on the CPU. 
The Taxonomy model running a 5 layered \gls{pam} took 132 hours before completing the 50 epochs.
Extended analysis and other models are investigated in \autoref{subsec:app_exten_models} and \autoref{app:cat_auth_pachinko}.

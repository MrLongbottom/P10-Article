\subsection{\acrlong{pam}}

\begin{frame}{\insertsubsection}{\gls{dag} structure}
    \begin{itemize}
    	\item<1-> Hierarchical topic model, capturing correlation between topics
    	\item<2-> Topic distributions can include other topics as well as words
    \end{itemize}
	
	\only<3->{
	\begin{figure}
		\centering
		\includegraphics[width=0.35 \textwidth]{../figures/pachinko_dag}
	\end{figure}}
\end{frame}

\begin{frame}{\insertsubsection}{Plate Notation}
	%\input{../figures/pachinko_original}
	\begin{figure}
			\centering
			\resizebox{0.45\columnwidth}{!}{%
			\input{../figures/pachinko_original}
			}
	\end{figure}
	\begin{itemize}
		\item Topic are sampled based on previous topics
	\end{itemize}
\end{frame}

\subsection{Results}

\begin{frame}{\insertsection}{Model Results}
	\begin{table}
		\centering
		\resizebox{.8\textwidth}{!}{
			\begin{tabular}{l|c|c}
				Topic Model & \makecell{Topic \\ Coherence} & \makecell{Topic \\ Difference} \\
				\midrule
				\Acrlong{lda} & 0.520 & 0.575 \\
				Author-topic model & 0.335 & 0.615 \\
				Category-topic model & 0.370 & 0.560 \\
				Taxonomy-topic model & \textbf{0.660} & \textbf{0.709} \\
			\end{tabular}
		}
	\end{table}
\end{frame}


\subsection{Taxonomy-Topic Model}

\begin{frame}{\insertsubsection}{Problems}
	Features needed to support taxonomy metadata:
	\begin{itemize}
		\item<1-> Hierarchical Structure
		\item<2-> Metadata Incorporation
		\item<3-> Partially Observed
		\item<4-> Multiple Taxonomies
	\end{itemize}
\end{frame}

\begin{frame}{\insertsubsection}{Metadata Incorporation}
	\begin{itemize}
		\item<1-> 1:1 mapping between some topics and taxonomy entries
		\only<2>{\newline\input{../figures/dag_structure}}
	\end{itemize}
\end{frame}

\begin{frame}{\insertsubsection}{Metadata Incorporation}
	\begin{itemize}
		\item<1-> 1:1 mapping between topics and taxonomy entires
		\item<2-> Restrict sampling from documents with known taxonomy
		\item<3-> Known taxonomies provide context to be fitted around
		\item<4-> Unobserved taxonomy sequences work like normal
		\item<5-> For documents with multiple sequences, we randomly choose one sequence for each word
	\end{itemize}
\end{frame}

\begin{frame}{\insertsubsection}{Plate Notation}
	\begin{columns}
		\begin{column}{0.45\textwidth}
			\begin{figure}
				\resizebox{\textwidth}{!}{%
					\input{../figures/pachinko_original}
				}
				\caption*{The LDA model}
			\end{figure}
		\end{column}
		\begin{column}{0.45\textwidth}
			\begin{figure}
				\resizebox{\textwidth}{!}{%
					\input{../figures/pachinko}
				}
				\caption*{The author-topic model}
			\end{figure}
		\end{column}
	\end{columns}
	\begin{itemize}
		\item One topic-distribution for each topic in previous layer
	\end{itemize}
\end{frame}

\subsection{Other Notable Models}

\begin{frame}{\insertsubsection}{\gls{pam} without metadata integration}
	\begin{tabular}{c|c|c|c|c|c|c}
		\makecell{Metadata \\ integration} & \multicolumn{5}{c}{\gls{dag} layer sizes} \vline & \makecell{Topic \\ Coherence} \\
		\hline
		$\checkmark$ & 1 & 4 & 32 & 90 & V & $0.66$\\
		- & & 1 & 100 & 90 & V & $0.67$ \\
	\end{tabular}
\end{frame}


\begin{frame}{\insertsubsection}{\gls{pam} without metadata integration}
	\begin{tabular}{c|c|c|c|c|c|c}
		\makecell{Metadata \\ integration} & \multicolumn{5}{c}{\gls{dag} layer sizes} \vline & \makecell{Elapsed Time \\ (hours)} \\
		\hline
		$\checkmark$ & 1 & 4 & 32 & 90 & V & 72\\
		- & 1 & 4 & 32 & 90 & V & 83\\
		- & & 1 & 100 & 90 & V &  128\\
		- & 1 & 100 & 100 & 90 & V & $\sim 6300$\\
	\end{tabular}
\end{frame}

\begin{frame}{\insertsubsection}{Category-Doc Models}
	\begin{figure}
		\centering
		\resizebox{0.3\columnwidth}{!}{%
			\input{../figures/author_category_plate.tex}
		}
	\end{figure}
	\begin{itemize}
		\item<1-> Multiple metadata types can be used in a single model
		\item<2-> Topics are sampled from a combination of two distributions
		\item<3-> This setup makes it possible to view each of the metadata entry's topic distributions seperately
		\item<4-> By having one topic-distribution for each document, one can combine the standard \gls{lda} with these metadata models as well
	\end{itemize}
\end{frame}

\begin{frame}{\insertsubsection}{Author-Category Models}
	\begin{figure}
		\centering
		\resizebox{0.35\columnwidth}{!}{%
			\input{../figures/author_doc_plate.tex}
		}
	\end{figure}
	\begin{itemize}
		\item<1-> By having one topic-distribution for each document, one can combine the standard \gls{lda} with metadata models
	\end{itemize}
\end{frame}

\begin{frame}{\insertsubsection}{Doc-Author \& Doc-Category Models}
	\begin{table}
		\centering
		\begin{tabular}{l|c|c}
			Topic Model & \makecell{Topic \\ Coherence} & \makecell{Topic \\ Difference} \\
			\midrule
			\Acrlong{lda} & 0.520 & 0.575 \\
			Author-topic model & 0.335 & 0.615 \\
			Category-topic model & 0.370 & 0.560 \\
			\textbf{Author-category model} & \textbf{0.390} & \textbf{0.537} \\
			\textbf{Author-doc model} & \textbf{0.543} & \textbf{0.574} \\
			\textbf{Category-doc model} &\textbf{ 0.530} & \textbf{0.575} \\
		\end{tabular}
	\end{table}
	\begin{itemize}
		\item<2-> Combining multiple topic-distributions can improve performance of topic models
		\item<3-> Also having a one topic-distribution for each document, makes the performance similar to the standard \gls{lda}
	\end{itemize}
\end{frame}

%\begin{frame}{\insertsubsection}{Author-\acrshort{pam} \& Category-\acrshort{pam} Models}
%	Author-\gls{pam} and Category-\gls{pam} \gls{dag} structure:
%	\begin{itemize}
%		\item Single topic layer with locking mechanism
%		\item One topic for each author / category
%		\item Single topic layer with no locking mechanism
%	\end{itemize}
%	\only<2->{\begin{table}
%		\centering
%		\begin{tabular}{l|c}
%			Topic Model & \makecell{Topic \\ Coherence} \\
%			\midrule
%			\Acrlong{lda} & 0.520 \\
%			Author-topic model & 0.335 \\
%			Category-topic model & 0.370 \\
%			Taxonomy-topic model & 0.660 \\
%			\textbf{Author \gls{pam}} & \textbf{0.598} \\
%			\textbf{Category \gls{pam}} & \textbf{0.585} \\
%			\textbf{\Acrlong{pam}} & \textbf{0.670} \\
%		\end{tabular}
%	\end{table}}
%	\begin{itemize}
%		\item<3-> The \gls{pam} models achieve better performance than \gls{lda} models
%	\end{itemize}
%\end{frame}

%\section{Information Retrieval Methods}
%
%%part
%
%\begin{frame}{\insertsection}
%    \begin{itemize}
%        \item Latent Dirichlet Allocation (LDA)
%        \item PageRank (PR)
%        \item Language Model (LM)
%        \item Term Frequency - Inverse Document Frequency (TF-IDF)
%        \item Best Match 25 (BM25)
%    \end{itemize}
%\end{frame}
%
%\subsection{Latent Dirichlet Allocation}
%\begin{frame}{\insertsubsection}{Motivation}
%    %Produce a generative process with the best possible chance of reconstructing the existing documents, using topics.
%    \begin{itemize}
%        \item Create a generative process to produce documents, based on topics
%        \item Fine-tune to maximize probability of generating the original documents
%        \item Use generated topics for calculating similarity
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{\insertsubsection}{Plate Notation}
%    \begin{figure}
%        \centering
%        \includegraphics[width = 1 \textwidth]{figures/lda_model.jpg}
%    \end{figure}
%    \begin{itemize}
%        \item $\alpha, \eta$ dirichlet distributions
%        \item $\theta, \beta$ multinomial distributions
%        \item $Z, W$ sampled topics and words
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{\insertsubsection}{Dirichlet Distributions}
%    \begin{figure}
%        \centering
%        \includegraphics[width = 0.65 \textwidth]{figures/lda_model.jpg}
%    \end{figure}
%    \begin{figure}
%        \centering
%        \includegraphics[width = 0.65 \textwidth]{figures/dirich.png}
%        \footnote{\url{https://mollermara.com/blog/lda/}}
%    \end{figure}
%    \begin{itemize}
%        \item<2> typical sample based on low alpha = $\{1,0,0\}$
%        \item<2> typical sample based on high alpha = $\{0.333, 0.333, 0.333\}$
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{\insertsubsection}{Multinomial Distributions}
%    \begin{figure}
%        \centering
%        \includegraphics[width = 0.65 \textwidth]{figures/lda_model.jpg}
%    \end{figure}
%    \begin{itemize}
%        \item Sample $N$ topics ($Z$) based on $\theta$
%        \item Sample $N$ words ($W$) based on $Z$ and $\beta$
%    \end{itemize}
%\end{frame}
%
%\begin{frame}{\insertsubsection}{Generation Probability}
%    \begin{align*}
%        & P(W,Z,\theta,\beta;\alpha,\eta) = \\
%        & \prod_{d=1}^{D}P(\theta_d;\alpha)
%        \prod_{k=1}^{K}P(\beta_k;\eta)
%        \prod_{n=1}^{N}P(Z_{d,n}|\theta_d) P(W_{d,n}|\beta, Z_{d,n})
%    \end{align*}
%\end{frame}
%
%% full example?
%% training?
%
%\subsection{PageRank}
%\begin{frame}{\insertsubsection}{Overview}
%    \begin{itemize}
%        \item Used to rank nodes in a graph
%        \item Underlying assumption: important nodes have in-going connections from other important nodes
%        \item Based on the 'random surfer' model
%    \end{itemize}
%    \only<2>{\begin{figure}
%        \centering
%        \includegraphics[width = 0.5 \textwidth]{figures/PageRank.jpg}
%        \footnote{\url{https://en.wikipedia.org/wiki/PageRank}}
%    \end{figure}}
%\end{frame}
%
%
%\begin{frame}{\insertsubsection}{Graph Construction}
%    \begin{itemize}
%        \item Used on adjacency matrix
%        \item Similarity between documents based on $\theta$
%        \begin{itemize}
%                    \item Calculated using Jensen-Shannon similarity
%        \end{itemize}
%        \item While fully connected each edge has a value which will influence the ranking
%    \end{itemize}
%\end{frame}

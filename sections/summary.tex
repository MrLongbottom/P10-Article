\section*{Summary}
\todo[inline]{Update text later depending on what changes we make to the paper}
%introduction
News is being broadcast every day around the world in the form of news articles, television, and newspapers, which supply people with the latest information.
Searching and categorizing the news is becoming a bigger problem since news is created at all times.
Topic modeling is an approach that takes a set of documents and generates topics that can be used for categorizing text.
We specifically look at extending the \gls{lda} model and the \gls{pam} with metadata.

In this paper we answer these research questions:
\begin{itemize}
	\item \textit{How can we establish models that incorporate metadata from the Nordjyske dataset?}
	\item \textit{How does including metadata within such models impact the resulting topics?}
\end{itemize}

%dataset
We work with a dataset Nordjyske, a Danish news agency.
This dataset contains $248,385$ articles from 2017 to 2019.
Basic preprocessing is done to make the data more applicable for topic models.
Each article includes three types of metadata: author, category, and taxonomy, that we chose to extend our topic models with.

The author field is for the author who has written the article.
This field is fully observed within the dataset, and each article only has a single author, so we do not account for multiple authors.
There are $227$ different authors within the dataset and after preprocessing $184$ remain.

The category field describes a variety of different aspects, but the categories are generally either about a specific newspaper or an overall subject.
This field is also fully observed within the dataset, and there are $58$ unique categories in the dataset before processing, and $34$ categories after preprocessing.

The taxonomy field describes a hierarchical sequence of the topical or geographical subject of the articles.
Each sequence consists of several taxonomy entries.
This field is only partially observed within the dataset, with ${\sim}25\%$ of the articles containing this field.
There are $1135$ different taxonomies and after preprocessing $345$ remain.

%models
The models we have made that include metadata for our evaluations are called the author-topic, category-topic, and taxonomy-topic model.
We also use a standard \gls{lda} model as a baseline for the performance of our models.
In \gls{lda}, $D$ is the number of documents in the corpus, $N_d$ is the number of words in document $d$, and $K$ is the number of topics.
Topics are represented as distributions over words and documents are represented as distributions of topics.

In the author-topic and category-topic models, there are no document-topic distributions $\theta$.
Instead, each of the $A$ authors and $C$ categories have their own topic distribution.
This is based on the assumption that authors prefer to write about specific topics and that categories of the articles were chosen based on the content of the finished article or that local newspapers have their own unique topic preferences.
For our category-topic model and author-topic model, each document $d$ is associated with one category $c_d$ from a set of categories $C$ and one author $a_d$ from a set of authors $A$, which is used when drawing a word topic.

To have our taxonomy-topic model handle the hierarchical structure of the taxonomy metadata field, we use a hierarchical topic model called the \acrfull{pam}.
Pachinko allocation generalizes \gls{lda}, making it possible to construct topic hierarchies based on any \gls{dag} structure.
\gls{pam} is a topic model focusing on finding topics of different abstraction levels and modeling the correlations between these topics.
Without modification, \gls{pam} finds topics with the same structure as our taxonomy, but the taxonomy values would be disregarded during training since it would generate new taxonomy sequences.
However, in our case, we have a dataset with a partially observed taxonomy field (${\sim}25\%$ of the documents), and we want to use the existing metadata information to estimate the topics quicker and more accurately.
To account for this, we only sample the unobserved nodes within the topic sequences.
For some of the dataset only the fourth layer is unobserved, but for the ${\sim}75\%$ documents without a taxonomy sequence, all layers are unobserved.
The observed taxonomy sequences are never sampled, hence they are 'locked' in place.
This creates a constant context for the taxonomy topics, which the documents with unobserved taxonomy sequences are fitted around.

%evaluation
The main metric used, in this experiment, is the topic coherence metric.
This metric indicates how semantically similar the top words within each topic are, and is an indication of the quality of the topics of a topic model.
The second metric used in the experiment is topic difference.
The topic difference is another metric that is used to check the quality of the topic model.
It is based on the assumption that a good topic model has little overlap between topics.

Before fitting our models, we search for the optimal hyperparameters for our models, since these can vary based on the dataset.
The hyperparameters we are testing in this grid search are the number of topics $K$ and an $\alpha$ and $\eta$ parameter.
We only run the grid search on the standard \gls{lda} model, with the assumption that the number of topics that perform well for this model, also performs well for the metadata models, when the same dataset is used.
Based on the topic coherence of the model we choose $K = 90$, $\alpha = 0.01$, and $\eta = 0.1$ as the hyperparameters for all models in the experiment.

For the results, the topic coherence of \gls{lda} is 0.520, author-topic is 0.335, category-topic is 0.370, and taxonomy-topic is 0.660.
This shows that the Author and Category-Topic models performed the worst in topic coherence, whereas the Taxonomy-Topic model is outperforming all other models.
However, the run time of the Taxonomy model is much higher than the standard \gls{lda}.

%discussion
In the analysis of the models, we explored the highest probable words from each model, except the taxonomy-topic model, that would appear in a chosen article.
It is seen that there is a large amount of overlap between the models, but the words that best summarized the article came from the \gls{lda} model.

For the author-topic and category-topic models, we are also able to analyze the similarity between author pairs and category pairs.
This similarity is calculated with the symmetric Kullback-Leibler divergence of either the author-topic distributions or the category-topic distributions.
This gives the intuition of how similar authors are based on the topics they write, and categories in the topics they cover.
It is seen that it is difficult to find specifically why authors are similar because they often write about many different subjects.
From the category similarities, there are obvious high similarity pairs, such as 'Sport-avis' and 'Mors√∏ Sport' where they both include sports articles.

For the taxonomy-topic model, it was found that the topics, in general, were the most understandable of all topic models, which made sense with it having the highest topic coherence.
The taxonomy-topic model was also able to separate words without meaning into their own topics, which allows the model to apply an extra layer of preprocessing, automatically filtering away irrelevant words into topics


%appendix
As further experimentation and exploration, we tested what would happen if we did further preprocessing.
We also tested the effect of including multiple topic distributions in our models and what would happen if we used the author and category metadata in the \gls{pam}.

For further preprocessing, we tested what effect including stemming in our dataset would have on the models.
With stemming on the \gls{lda} model, it was seen that there were a lot fewer unique words.
When looking at the topics of the model, it was seen that they generally did not change much, but may be slightly more understandable with fewer words with the same meaning.

One of the new models we made is called the author-category model.
This model includes both an author-topic distribution and a category-topic distribution and uses both to draw word topics.
This model has only a slightly higher topic coherence of 0.390 compared to the author-topic and category-topic model, and the topics are still not very understandable.

We also made two other models called Author-Doc and Category-Doc.
These models are made by combining the \gls{lda} model with the author-topic and category-topic mode, and also use both distributions to draw word topics.
We run these models using the same hyperparameters as all the other models and they get similar results to the standard \gls{lda} model.
Author-doc gets a topic coherence of 0.543 and category-doc gets 0.530.

Finally, we tested what would happen if we ran \gls{pam} with just the author and category metadata.
A four-layer \gls{pam} was used for both of these models, but otherwise, the same structure is used as with the taxonomy-topic model.
The author \gls{pam} gets a topic coherence of 0.598 and the category \gls{pam} gets 0.585.
These models achieved better results than the previous author and category models and the \gls{lda} model but had lower topic coherence compared to the taxonomy-topic model.

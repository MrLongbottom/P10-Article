\section{Evaluation}\label{sec:experiment}
In this section, we evaluate the topic models previously defined.
We also define the evaluation metrics, and how the hyperparameters for the models were chosen.
Lastly, the results of the evaluations are also shown.

\subsection{Models}\label{sec:experiment_models}
A list of different topic models is evaluated, each using different metadata from the dataset.
The main difference between the models is how they draw a specific topic for a word.
As detailed in \autoref{sec:plate_notation}, the main models used in this experiment are:
\begin{description}
	\item[\Acrlong{lda}] Standard \gls{lda}, which uses document-topic distributions.
	\item[Author-Topic Model]\cite{author_topic_2012} An \gls{lda} based model, which uses author-topic distributions.
	\item[Category-Topic Model] An \gls{lda} based model, which uses category-topic distributions.
	\item[Taxonomy-Topic Model] A \acrlong{pam} which uses hierarchical taxonomy information.
\end{description}

By extension of these models, combinations of the models are also evaluated.
As detailed in \autoref{sec:combinations}, these combinations are:
\begin{itemize}
	\item Author-Category
	\item Author-Taxonomy
	\item Category-Taxonomy
	\item Author-Category-Taxonomy
\end{itemize}
These model combinations should give insight into what a model learns when multiple metadata have influence on the topics chosen.

\subsection{Evaluation Metrics}\label{sec:experiment_metrics}
All models previously mentioned are evaluated on the following evaluation metrics.
The main metric used, in this experiment, is the topic coherence metric.
This metric indicates how semantically similar the top words within each topic are, and will be an indication of the quality of the topics a topic model generates~\cite{topic_coherence_2015}.
There are many different methods for calculating topic coherence, but for this paper we will be using $C_v$ coherence.
$C_v$ is calculated using the following steps, as presented by~\citet{Syed2017coherence}:
\begin{enumerate}
	\item Topic-word segmentation into word set pairs
	\item Word and word pair probability calculation
	\item Word set confirmation measure
	\item Aggregation of confirmation measures
\end{enumerate}
The intuition behind this, is to measure the co-occurrence of the top words within each topic, which is done in step 1 and 2.
From step 1 and 2, vectors are created based on the co-occurrence, which we measure the cosine similarity between to see how similar these vectors are.
We can calculate the $C_v$ measure, by summing the distances between these vectors. 

We need to create a set of word pairs $W'$, for each word in the top-N most probable words $W$, where $W = \{W_1, \dots, W_N\}$.
We use $W'$ to create an \gls{pmi} matrix which in turn is used to construct the \acrshort{npmi} matrix used in \autoref{eq:coherence_1}.

To calculate the probability of words, we use a sliding window of size $s$, to create a set of sub documents $D_s$ over the document set $D$.
$p(w_i)$, is the number of sub documents in which the word $w_i$ occurs divided by $|D_s|$.
$p(w_i, w_j)$, it is the number of sub documents in which both words occur divided by $|D_s|$. 
\begin{equation}\label{eq:pmi}
	\text{PMI}(w_i,w_j) = \log\frac{p(w_i,w_j) + \epsilon}{p(w_i)\cdot p(w_j)}
\end{equation}
where $\epsilon$ is the number of word pairs in $W'$. 
The \gls{npmi} matrix describes how much each word occurs alone versus together with other words. 
To calculate the NPMI matrix, we need to normalize the \gls{pmi} matrix and this is done by dividing the \gls{pmi} with $-log(P(w_i,w_j) + \epsilon)$.
This is a -1 to 1 normalization.
\begin{equation}\label{eq:coherence_2}
	\text{NPMI}(w_i,w_j) =  \frac{\text{PMI}(w_i, w_j)}{-log(p(w_i,w_j) + \epsilon)}
\end{equation}
Now that we have calculated the \gls{npmi} matrix, we sum it to create a context vector, which describes how much each top word co-occurs with the other words in $W'$.
\begin{equation}\label{eq:coherence_1}
	\overrightarrow{v}(W') = \left\{ \sum_{w_i \in W'} \text{NPMI}(w_i, w_j) \right\}_{j=1,\dots,|W|}
\end{equation}

We now have a context vector for each top-N word in our topic and we want to know how different these vectors are.
This is done to measure how similar the topics are.
\begin{equation}\label{eq:coherence_3}
	\phi_{S_i}(\overrightarrow{u}, \overrightarrow{w}) = \frac
	{\sum_{i = 1}^{|W|} u_i \cdot w_i}
	{\|\overrightarrow{u}\|_2 \cdot \|\overrightarrow{w}\|_2}
\end{equation}
\begin{equation}\label{eq:coherence_4}
	C_v = \frac{1}{|S|}\sum_{|S|}^{i=1}\phi_{S_i}
\end{equation}
\noindent where $W$ is a set of the top-N words in a topic, and $S$ is a set of all possible pairs $(W',W*)$ where $W'\in W$ and $W* = W$.
This is the segmentation.
NPMI is the normalized pointwise mutual information.
$\overrightarrow{v}(W')$ is the context vector for $W'$ which is made by pairing them to all words in $W$.\vejleder{skal forklares ydereligere}
$\phi_{S_i}$ is the confirmation measure for a segmentation pair $S_i = (W',W*)$, that describes how much $W*$ supports $W'$.\vejleder{skal forklares ydereligere}
It is calculated by the cosine similarity of all context vectors $\phi_{S_i}(\overrightarrow{u},\overrightarrow{w})$ in $S_i$, with $\overrightarrow{v}(W') \in \overrightarrow{u}$ and $\overrightarrow{v}(W*) \in \overrightarrow{w}$.
$\epsilon$ is used to avoid $log(0)$ and $\gamma$ is used to further weigh high NPMI values higher.

The second metric used in the experiment is perplexity.
Perplexity is used as a metric, to show how well a model can predict new test samples $w_d$.
But because perplexity is not specific to topic models, and by itself does not give an indication of how coherent topics are, it is mainly used as a secondary evaluation~\cite{tea_leaves}.
To calculate perplexity, we first need to compute the log-likelihood of $w_d$, which is done in:
\begin{equation}\label{eq:likelihood}
	\mathcal{L}(w_d) = \log p(w_d|\Phi) = \sum_{d} \log p(w_d|\Phi)
\end{equation}
\noindent where $\Phi$ is the topic-word matrix.
The perplexity measure is then calculated as follows:
\begin{equation}
	\emph{Perplexity}(w_d) = exp \{-\frac{\mathcal{L}(w_d)}{W}\}
\end{equation}
\noindent where W is the number of words \cite{de2008evaluating}.

Topic difference is another metric that is used to check the quality of the topic model.
It is based on the assumption that a good topic model will have little overlap between topics.
It is therefore not necessarily the best measure of the final quality of a topic model, but it can show potential problems with a model.
\begin{equation}
	\emph{TopicDifference} = \frac{1}{K \cdot K} \sum_{i=1}^{K} \sum_{j=1}^{K} JS(\beta_{i},\beta_{j})
\end{equation}
\noindent where $JS$ is Jensen-Shannon distance, $K$ is the number of topics, and $\beta_{k}$ is the topic-word distribution for topic $k$.
If the total sum is not averaged out, this measure can also be used to indicate convergence of the model.

Finally, we also perform human evaluation of the topics generated.
\todo[inline]{How/if we do this is still to be discussed}

\subsection{Grid Search}\label{sec:experiment_gridsearch}
To find the optimal hyperparameter values for the models, we run a grid search.
In the grid search, different values of $K$, $\alpha$, and $\eta$ are tested to find the best performing model.
Specifically, we run two rounds of grid search.
In the first round, the number of topics $K$ we test are the values of $K_1$, as seen in \autoref{tab:gridsearch}, with randomly chosen $\alpha$ and $\eta$ values for each $K$ value.
This creates much fewer runs of the grid search to start with, and eliminates hyperparameter values that give clearly worse models.
In the second round, the number of topics $K$ we test are the values of $K_2$, with all combinations of $\alpha$ and $\eta$, except for those with values of $0.001$, since these models gave much worse scores.
The hyperparameter values that are tested, are shown in \autoref{tab:gridsearch}.

We only run the grid search on the standard \gls{lda} model, with the assumption that the number of topics that performs well for this model, also performs well for the metadata models, when the same dataset is used.
To evaluate the \gls{lda} models, we measure the topic coherence of a model after training it on the dataset for 50 epochs, and the hyperparameters of the model with the highest score are then used for the models in the rest of the experiment.

Based on the topic coherence of the model, we choose $K = 90$, $\alpha = 0.01$, and $\eta = 0.1$ as the hyperparameters for all models in the experiment.

\input{tables/grid_search.tex}

\subsection{Results}\label{sec:results}

\input{tables/metric_results.tex}

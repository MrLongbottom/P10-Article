\section{Evaluation}\label{sec:experiment}
In this section, we describe the evaluation metrics and how hyperparameters for the models are chosen.
Lastly, an overview of the results is given.

For our evaluation we train four models on our news article dataset.
The standard \gls{lda} model is used as a baseline, and our models: the author-topic model, the category-topic model, and the taxonomy-topic model, are tested.
The main differences between the models are how they draw a specific topic for a word, and in the taxonomy-topic model, a hierarchical structure is used.

\subsection{Evaluation Metrics}\label{sec:experiment_metrics}
The main metric used in this evaluation is topic coherence\cite{Syed2017coherence}.
This metric indicates how semantically similar the top words within each topic are and can therefore be used as an indication of the topic quality within a model~\cite{topic_coherence_2015}.
There are different ways of calculating topic coherence.
We are using $C_v$ topic coherence, for this paper.
The intuition is to calculate the degree of semantic similarity between highly probable words in a topic.
Topic coherence ranges between zero and one.
This evaluation metric is explained further in Appendix \autoref{app:topic_coherence}.

The second metric used in the evaluation is topic difference.
Topic difference is another metric that is used to check the quality of the topic model.
It is based on the assumption that a good topic model has little overlap between topics.
It is not the best measure of the final quality of a topic model as non-coherent topics can have little overlap.
However, a low topic difference can be an indicator of potential problems within a model.
Topic difference is calculated using the following equation:

\begin{equation}
	\emph{TopicDifference} = \frac{1}{K \cdot K} \sum_{i=1}^{K} \sum_{j=1}^{K} JS(\beta_{i},\beta_{j})
\end{equation}
\noindent where $JS$ is the Jensen-Shannon distance, $K$ is the number of topics, and $\beta_{k}$ is the topic-word distribution for topic $k$.
Topic difference ranges between zero and one.

\subsection{Grid Search}\label{sec:experiment_gridsearch}
To find the optimal hyperparameter values for the models, we run two rounds of grid searches.
To find the best-performing model we test different values for the number of topics $K$ and the two Dirichlet priors $\alpha$ and $\eta$, for the document-topic/metadata-topic distributions and topic-word distribution, respectively.
In the first round, the number of topics $K$ we test are the values of $K_1$, as seen in \autoref{tab:gridsearch}, with a single randomly chosen $\alpha$ and $\eta$ value for each $K$ value.
This creates much fewer runs of the grid search to start with and eliminates hyperparameter values that give worse models.
In the second round, the number of topics $K$ we test are the values of $K_2$ with all combinations of $\alpha$ and $\eta$ except for those with values of $0.001$, since these models gave much worse scores.

We only run the grid search on the standard \gls{lda} model, with the assumption that the number of topics that perform well for this model also performs well for the metadata models when the same dataset is used.\footnote{Initial testing on the category-topic model indicates that this assumption holds true.}
To evaluate the \gls{lda} models during the grid search, we measure the topic coherence of a model after training it on the dataset for 50 epochs.
The hyperparameters of the model with the highest score are then used for the models in the rest of the experiment.

Based on the topic coherence of the best-performing model, we choose $K = 90$, $\alpha = 0.01$, and $\eta = 0.1$ as the hyperparameters for all models in the experiment.
More details on how we chose these values are given in Appendix \autoref{sec:appendix_grid_search}.

\input{tables/grid_search.tex}
\todo[inline]{fix dots so they arent centered}

\subsection{Overview of results}\label{sec:results}

\input{tables/metric_results.tex}

\section{Evaluation}\label{sec:experiment}
In this section, we define the evaluation metrics and how hyperparameters for the models were chosen.
Lastly, an overview of the results is given.

\subsection{Models}\label{sec:experiment_models}
A list of different topic models is evaluated each using different metadata from the dataset.
The main difference between the models is how they draw a specific topic for a word.
As detailed in \autoref{sec:plate_notation} the main models used in this experiment are:
\begin{description}
	\item[\Acrlong{lda}] Standard \gls{lda} which uses document-topic distributions.
	\item[Author-topic model]\cite{author_topic_2012} An \gls{lda} based model, which uses author-topic distributions.
	\item[Category-topic model] An \gls{lda} based model which uses category-topic distributions.
	\item[Taxonomy-topic model] A \acrlong{pam} which uses hierarchical taxonomy information.
\end{description}
\todo[inline]{Føler ikke dette afsnit giver nok til at fylde så meget. Tænker subsection kan fjernes, og at det meste af teksten kan forkortes til 1-2 sætninger. Itemizen med beskrivelse behøves ikke, men modellerne brugt kan nævnes i en af sætningerne. husk at emphasize at det er vores modeller}

\subsection{Evaluation Metrics}\label{sec:experiment_metrics}
The main metric used in this evaluation is topic coherence\cite{Syed2017coherence}.
This metric indicates how semantically similar the top words within each topic are and can therefore be used as an indication of the topic quality within a model~\cite{topic_coherence_2015}.
There are different ways of calculating topic coherence.
We are using $C_v$ topic coherence, for this paper.
The intuition is to calculate the degree of semantic similarity between highly probable words in a topic.
Topic coherence ranges between zero and one.
This evaluation metric is explained further in Appendix \autoref{app:topic_coherence}.

The second metric used in the evaluation is topic difference.
Topic difference is another metric that is used to check the quality of the topic model.
It is based on the assumption that a good topic model will have little overlap between topics.
It is not the best measure of the final quality of a topic model as non-coherent topics can have little overlap.
However, a low topic difference can be an indicator of potential problems within a model.
Topic difference is calculated using the following equation:

\begin{equation}
	\emph{TopicDifference} = \frac{1}{K \cdot K} \sum_{i=1}^{K} \sum_{j=1}^{K} JS(\beta_{i},\beta_{j})
\end{equation}
\noindent where $JS$ is the Jensen-Shannon distance, $K$ is the number of topics, and $\beta_{k}$ is the topic-word distribution for topic $k$.
Topic difference ranges between zero and one.

\subsection{Grid Search}\label{sec:experiment_gridsearch}
To find the optimal hyperparameter values for the models, we run two rounds of grid searches.
To find the best-performing model we test different values for the number of topics $K$ and the two Dirichlet priors $\alpha$ and $\eta$, for the document-topic/metadata-topic distributions and topic-word distribution, respectively.
In the first round, the number of topics $K$ we test are the values of $K_1$, as seen in \autoref{tab:gridsearch}, with a single randomly chosen $\alpha$ and $\eta$ value for each $K$ value.
This creates much fewer runs of the grid search to start with and eliminates hyperparameter values that give worse models.
In the second round, the number of topics $K$ we test are the values of $K_2$ with all combinations of $\alpha$ and $\eta$ except for those with values of $0.001$, since these models gave much worse scores.

We only run the grid search on the standard \gls{lda} model, with the assumption that the number of topics that perform well for this model also performs well for the metadata models when the same dataset is used.\footnote{Initial testing on the category-topic model indicates that this assumption holds true.}
To evaluate the \gls{lda} models during the grid search, we measure the topic coherence of a model after training it on the dataset for 50 epochs.
The hyperparameters of the model with the highest score are then used for the models in the rest of the experiment.

Based on the topic coherence of the best-performing model, we choose $K = 90$, $\alpha = 0.01$, and $\eta = 0.1$ as the hyperparameters for all models in the experiment.

\input{tables/grid_search.tex}
\todo[inline]{fix dots so they arent centered}

\subsection{Overview of results}\label{sec:results}

\input{tables/metric_results.tex}

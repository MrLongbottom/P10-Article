\section{Evaluation}\label{sec:experiment}
In this section, we evaluate the topic models previously defined.
We also define the evaluation metrics, and how the hyperparameters for the models were chosen.
Lastly, the results of the evaluations are also shown.

\subsection{Models}\label{sec:experiment_models}
A list of different topic models is evaluated, each using different metadata from the dataset.
The main difference between the models is how they draw a specific topic for a word.
As detailed in \autoref{sec:plate_notation}, the main models used in this experiment are:
\begin{description}
	\item[\Acrlong{lda}] Standard \gls{lda}, which uses document-topic distributions.
	\item[Author-Topic Model]\cite{author_topic_2012} An \gls{lda} based model, which uses author-topic distributions.
	\item[Category-Topic Model] An \gls{lda} based model, which uses category-topic distributions.
	\item[Taxonomy-Topic Model] A \acrlong{pam} which uses hierarchical taxonomy information.
\end{description}

\subsection{Evaluation Metrics}\label{sec:experiment_metrics}
All models previously mentioned are evaluated on the following evaluation metrics.

The main metric used, in this experiment, is the topic coherence metric.
This metric indicates how semantically similar the top words within each topic are, and will be an indication of the quality of the topics of a topic model~\cite{topic_coherence_2015}.
There are many different methods for calculating topic coherence, but for this paper we will be using $C_v$ coherence.
$C_v$ is calculated using the following steps, as presented by~\citet{Syed2017coherence}:
\begin{enumerate}
	\item Topic-word segmentation into word set pairs
	\item Word and word pair probability calculation
	\item Word set confirmation measure
	\item Aggregation of confirmation measures
\end{enumerate}
\vejleder[inline]{hvad er intuitionen? hvorfår gør man det på denne måde?}
The equations, explanations, and values of the hyperparameters in this section are based on~\citet{Syed2017coherence} and the \textit{gensim} python package\footnote{\url{https://radimrehurek.com/gensim/}}.

For segmentation, a set of word pairs $S$ is created, which pairs each word in the top-N most probable words $W$ in a specific topic $t$ with all other words in $W$.
$S$ is defined by \autoref{eq:set_of_word_pairs}.

\begin{equation}\label{eq:set_of_word_pairs}
	S = \{(W', W*)|W' = \{w_i\};w_i \in W;W* = W\}
\end{equation}

Before calculating word probabilities, a sliding window of size $s$ where $s =110$ is used to create a set of subdocuments $D_s$ over the document set $D$.
These subdocuments are used rather than the normal documents to capture some degree of word proximity.
Word probabilities are calculated based on how many documents, within the document set $D_s$, they occur in.
$p(w_i)$ is the number of subdocuments in which the word $w_i$ occurs divided by $|D_s|$.
$p(w_i, w_j)$ is the number of subdocuments in which both words occur divided by $|D_s|$. 

We create a \gls{npmi} matrix of size $|W|\times|W|$, with one entry per word pair combination in $W$.
\begin{equation}\label{eq:coherence_2}
	\text{NPMI}(w_i,w_j) =  \frac{\log\frac{p(w_i,w_j) + \epsilon}{p(w_i)\cdot p(w_j)}}{-log(p(w_i,w_j) + \epsilon)}
\end{equation}
\noindent where $\epsilon$ is a low number ($10^{-12}$) used to avoid $log(0)$.
The \gls{npmi} matrix describes how much each word occurs alone versus together with other words.
Each value is between $-1$ and $1$, with $-1$ meaning that the words never occur together and 1 meaning that they only occur together.

After having calculated the \gls{npmi} matrix, we construct context vectors for both elements $W'$ and $W*$ in each word-pair $S_i$, by summing over the rows of the \gls{npmi} matrix.
This summation describes how much each top word co-occurs with the other words in $W$.
\begin{equation}\label{eq:coherence_1}
	\overrightarrow{v}(W') = \left\{ \sum_{w_i \in W'} \text{NPMI}(w_i, w_j)^{\gamma} \right\}_{j=1,\dots,|W|}
\end{equation}
\noindent where $\gamma$ can be used to further prioritize higher values.
For this paper we use $\gamma = 1$, as recommended by \citet{Syed2017coherence}.

We now have a pair of context vectors for each word pair $S_i$ and we want to know how different these vectors are.
This is calculated using cosine similarity as a confirmation measure.
\begin{equation}\label{eq:coherence_3}
	\phi_{S_i}(\overrightarrow{u}, \overrightarrow{w}) = \frac
	{\sum_{i = 1}^{|W|} u_i \cdot w_i}
	{\|\overrightarrow{u}\|_2 \cdot \|\overrightarrow{w}\|_2}
\end{equation}

\noindent where $\overrightarrow{u}$ is the context vector $\overrightarrow{v}(W')$, and $\overrightarrow{w}$ is the context vector $\overrightarrow{v}(W^*)$.

Lastly, the confirmation measures are aggregated using the arithmetic mean, to achieve the coherence value of topic $t$.

\begin{equation}\label{eq:coherence_4}
	C_v = \frac{1}{|S|}\sum_{i=1}^{|S|}\phi_{S_i}
\end{equation}

The second metric used in the experiment is perplexity.
Perplexity is used as a metric, to show how well a model can predict new test samples $w_d$.
But because perplexity is not specific to topic models, and by itself does not give an indication of how coherent topics are, it is mainly used as a secondary evaluation~\cite{tea_leaves}.
To calculate perplexity, we first need to compute the log-likelihood of $w_d$, which is done in:
\begin{equation}\label{eq:likelihood}
	\mathcal{L}(w_d) = \log p(w_d|\Phi) = \sum_{d} \log p(w_d|\Phi)
\end{equation}
\noindent where $\Phi$ is the topic-word matrix.
The perplexity measure is then calculated as follows:
\begin{equation}
	\emph{Perplexity}(w_d) = exp \{-\frac{\mathcal{L}(w_d)}{W}\}
\end{equation}
\noindent where W is the number of words \cite{de2008evaluating}.

Topic difference is another metric that is used to check the quality of the topic model.
It is based on the assumption that a good topic model will have little overlap between topics.
It is therefore not necessarily the best measure of the final quality of a topic model, but it can show potential problems with a model.
\begin{equation}
	\emph{TopicDifference} = \frac{1}{K \cdot K} \sum_{i=1}^{K} \sum_{j=1}^{K} JS(\beta_{i},\beta_{j})
\end{equation}
\noindent where $JS$ is Jensen-Shannon distance, $K$ is the number of topics, and $\beta_{k}$ is the topic-word distribution for topic $k$.
If the total sum is not averaged out, this measure can also be used to indicate convergence of the model.

Finally, we also perform human evaluation of the topics generated.
\todo[inline]{How/if we do this is still to be discussed}

\subsection{Grid Search}\label{sec:experiment_gridsearch}
To find the optimal hyperparameter values for the models, we run a grid search.
In the grid search, different values of $K$, $\alpha$, and $\eta$ are tested to find the best performing model.
Specifically, we run two rounds of grid search.
In the first round, the number of topics $K$ we test are the values of $K_1$, as seen in \autoref{tab:gridsearch}, with randomly chosen $\alpha$ and $\eta$ values for each $K$ value.
This creates much fewer runs of the grid search to start with, and eliminates hyperparameter values that give clearly worse models.
In the second round, the number of topics $K$ we test are the values of $K_2$, with all combinations of $\alpha$ and $\eta$, except for those with values of $0.001$, since these models gave much worse scores.
The hyperparameter values that are tested, are shown in \autoref{tab:gridsearch}.

We only run the grid search on the standard \gls{lda} model, with the assumption that the number of topics that performs well for this model, also performs well for the metadata models, when the same dataset is used.
To evaluate the \gls{lda} models, we measure the topic coherence of a model after training it on the dataset for 50 epochs, and the hyperparameters of the model with the highest score are then used for the models in the rest of the experiment.

Based on the topic coherence of the model, we choose $K = 90$, $\alpha = 0.01$, and $\eta = 0.1$ as the hyperparameters for all models in the experiment.

\input{tables/grid_search.tex}

\subsection{Results}\label{sec:results}

\input{tables/metric_results.tex}

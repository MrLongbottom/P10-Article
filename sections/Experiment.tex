\section{Evaluation}\label{sec:experiment}
In this section, we evaluate the topic models previously defined.
We also define the evaluation metrics, and how the hyperparameters for the models were chosen.
Lastly, the results of the evaluations are also shown.

\subsection{Models}\label{sec:experiment_models}
A list of different topic models is evaluated, each using different metadata from the dataset.
The main difference between the models is how they draw a specific topic for a word.
As detailed in \autoref{sec:plate_notation}, the main models used in this experiment are:
\begin{description}
	\item[\Acrlong{lda}] Standard \gls{lda}, which uses document-topic distributions.
	\item[Author-Topic Model]\cite{author_topic_2012} An \gls{lda} based model, which uses author-topic distributions.
	\item[Category-Topic Model] An \gls{lda} based model, which uses category-topic distributions.
	\item[Taxonomy-Topic Model] A \acrlong{pam} which uses hierarchical taxonomy information.
\end{description}

By extension of these models, combinations of the models are also evaluated.
As detailed in \autoref{sec:combinations}, these combinations are:
\begin{itemize}
	\item Author-Category
	\item Author-Taxonomy
	\item Category-Taxonomy
	\item Author-Category-Taxonomy
\end{itemize}
These model combinations should give insight into what a model learns when multiple metadata have influence on the topics chosen.

\subsection{Evaluation Metrics}\label{sec:experiment_metrics}
All models previously mentioned are evaluated on the following evaluation metrics.

The main metric used, in this experiment, is the topic coherence metric.
This metric indicates how semantically similar the top words within each topic are, and will be an indication of the quality of the topics of a topic model~\cite{topic_coherence_2015}.
There are many different methods for calculating topic coherence, but for this paper we will be using $C_v$ coherence.
$C_v$ is calculated using the following steps, as presented by~\citet{Syed2017coherence}:
\begin{enumerate}
	\item Topic-word segmentation into word set pairs
	\item Word and word pair probability calculation
	\item Word set confirmation measure
	\item Aggregation of confirmation measures
\end{enumerate}
The intuition is to calculate the degree of semantic similarity between highly probable words in a topic.
This intuition, and each step, is explained further in Appendix \autoref{app:topic_coherence}.

The second metric used in the experiment is perplexity.
Perplexity is used as a metric, to show how well a model can predict new test samples $w_d$.
But because perplexity is not specific to topic models, and by itself does not give an indication of how coherent topics are, it is mainly used as a secondary evaluation~\cite{tea_leaves}.
To calculate perplexity, we first need to compute the log-likelihood of $w_d$, which is done in:
\begin{equation}\label{eq:likelihood}
	\mathcal{L}(w_d) = \log p(w_d|\Phi) = \sum_{d} \log p(w_d|\Phi)
\end{equation}
\noindent where $\Phi$ is the topic-word matrix.
The perplexity measure is then calculated as follows:
\begin{equation}
	\emph{Perplexity}(w_d) = exp \{-\frac{\mathcal{L}(w_d)}{W}\}
\end{equation}
\noindent where W is the number of words \cite{de2008evaluating}.

Topic difference is another metric that is used to check the quality of the topic model.
It is based on the assumption that a good topic model will have little overlap between topics.
It is therefore not the best measure of the final quality of a topic model, but a low topic difference will indicate potential problems with a model.
\begin{equation}
	\emph{TopicDifference} = \frac{1}{K \cdot K} \sum_{i=1}^{K} \sum_{j=1}^{K} JS(\beta_{i},\beta_{j})
\end{equation}
\noindent where $JS$ is Jensen-Shannon distance, $K$ is the number of topics, and $\beta_{k}$ is the topic-word distribution for topic $k$.
If the total sum is not averaged out, this measure can also be used to indicate convergence of the model.

Finally, we also perform human evaluation of the topics generated.
\todo[inline]{How/if we do this is still to be discussed}

\subsection{Grid Search}\label{sec:experiment_gridsearch}
To find the optimal hyperparameter values for the models, we run a grid search.
In the grid search, different values of $K$, $\alpha$, and $\eta$ are tested to find the best performing model.
Specifically, we run two rounds of grid search.
In the first round, the number of topics $K$ we test are the values of $K_1$, as seen in \autoref{tab:gridsearch}, with randomly chosen $\alpha$ and $\eta$ values for each $K$ value.
This creates much fewer runs of the grid search to start with, and eliminates hyperparameter values that give clearly worse models.
In the second round, the number of topics $K$ we test are the values of $K_2$, with all combinations of $\alpha$ and $\eta$, except for those with values of $0.001$, since these models gave much worse scores.
The hyperparameter values that are tested, are shown in \autoref{tab:gridsearch}.

We only run the grid search on the standard \gls{lda} model, with the assumption that the number of topics that performs well for this model, also performs well for the metadata models, when the same dataset is used.
To evaluate the \gls{lda} models, we measure the topic coherence of a model after training it on the dataset for 50 epochs, and the hyperparameters of the model with the highest score are then used for the models in the rest of the experiment.

Based on the topic coherence of the model, we choose $K = 90$, $\alpha = 0.01$, and $\eta = 0.1$ as the hyperparameters for all models in the experiment.

\input{tables/grid_search.tex}

\subsection{Results}\label{sec:results}

\input{tables/metric_results.tex}

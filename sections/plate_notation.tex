\section{Our Topic Models}\label{sec:plate_notation}
In this section, we detail the modifications that we have implemented to the existing models, outlined in \autoref{sec:preliminaries}.
We present three models, one for each of the metadata fields detailed in \autoref{sec:dataset}.
Two of the models: the Author and Category metadata models are based on the Author-topic \gls{lda} explained in \autoref{subsec:auth_prelim}.
Lastly the Taxonomy metadata model is based on \gls{pam}, which is explained in \autoref{subsec:pachinko_prelim}.

\subsection{Author-Topic \gls{lda} and Category-Topic \gls{lda}}
We model both of the metadata fields 'Author' and 'Category' similarly to the model by \citet{author_topic_2012} presented in \autoref{subsec:auth_prelim}.
The category model builds on the assumption that categories of the articles were chosen based on the content of the finished article and that local newspapers have their own unique topic preferences.
We find this model to be generally applicable to most metadata information, assuming that the metadata is either chosen based on the text of the documents (as with the category metadata) or that it has some impact on the text of the document (as with the author metadata).

For our category-topic model and our author-topic model, each document $d$ is associated with only one category $c_d$ from a set of categories $C$ and only one author $a_d$ from a set of authors $A$, rather than a vector.
This is due to our dataset never having more than one author or category for each document.
Unless otherwise stated, future mentions of the author-topic model are to our implementation of this model, rather than the model presented by \citet{author_topic_2012}.
The plate notation for our category and author \gls{lda} models can be seen in \autoref{fig:metadata_lda}.

\begin{figure}[ht]
	\centering
	\begin{subfigure}{0.425\columnwidth}
		\centering
		\resizebox{\textwidth}{!}{%
		\input{figures/category_plate}
		}
		\caption{Category \gls{lda}.}
		\label{fig:category_lda}
	\end{subfigure}
	\hspace{2em}
	\begin{subfigure}{0.425\columnwidth}
		\centering
		\resizebox{\textwidth}{!}{%
			\input{figures/author_plate}
		}
		\caption{Author \gls{lda}.}
		\label{fig:author_lda}
	\end{subfigure}
	\caption{Plate notation for the metadata \gls{lda} models.}
	\label{fig:metadata_lda}
\end{figure}

\subsection{Pachinko Allocation}
In order to handle the hierarchical structure of the taxonomy metadata field, we use a hierarchical topic model, namely the \acrfull{pam} from \citet{li2006pachinko}.

In this paper, we construct a five-level \gls{pam}, following the format presented by \citet{li2006pachinko}.
The \gls{dag} structure used in this paper is in \autoref{fig:pachinko_dag}, where each layer has been named.
The plate notation for this model is shown in \autoref{fig:pachinko}

Some layers in our \gls{dag} structure are duplicates of layers from the taxonomy field within our dataset.
This allows us to have some nodes represent a topic based on a specific taxonomy.
To make the algorithm construct the topics to be based on our taxonomies, we introduce a novel locking mechanism for the Gibbs sampler which we use to run \gls{pam}.
This mechanism is discussed further in \autoref{subsec:mod_pachinko}.

The first layer of the \gls{dag} is the root layer.
The last layer is the word layer consisting of one node for each word in the vocabulary of our corpus.
The second and third layers will be constructed based on the entries of the first two positions in our taxonomy metadata, meaning there will be one node for each unique sub-taxonomy entry that is in the first or second position in the taxonomy sequence (e.g. "STEDER" and "Danmark", which is taken from "STEDER/Danmark/Aalborg", but not "Aalborg" since it is in the third position).
We only use the first two layers for this, as these are among the most informative, and because introducing even more layers would slow down the training significantly, since the probability of all possible combinations of topics needs to be sampled for every word during training.
From our experiments, the training time for 50 epochs, increases from 12 hours to 130 hours between four and five level pachinko.
The fourth layer consists of $K = 90$ 'blank' topics, as with the other models we present in this paper.
This layer is added so that the model can construct its own topics based on the higher-level topics learned from our taxonomy metadata.

We use Gibbs sampling for performing inference.
For each word, a chain of topics is sampled by calculating the probability of all combinations of topics and making a weighted sample.
The probability of each topic combination is calculated using the joint probability of the topics, as presented in \autoref{eq:pachinko_gibbs}.

\begin{equation}\label{eq:pachinko_gibbs}
	\begin{split}
		P(Z_{w2} = t_a, Z_{w3} = t_b, Z_{w4} = t_c | \textbf{D}, z_{-w}, \alpha, \beta) \propto \\
		\frac{n_{1a}^d + \alpha_{1a}}{n_1^d + \sum_{a'} \alpha_{1a'}} \times
		\frac{n_{ab}^d + \alpha_{ab}}{n_a^d + \sum_{b'} \alpha_{ab'}}  \times 
		\frac{n_{bc}^d + \alpha_{bc}}{n_{b}^d + \sum_{c'} \alpha_{bc'}} \\ \times 
		\frac{n_{cw}^d + \beta_{w}}{n_{c} + \sum_{m} \beta_{m}} 
	\end{split}
\end{equation}
As in \citet{li2006pachinko}, $Z_{w2}$, $Z_{w3}$, $Z_{w4}$ are topic assignments for the three middle layers of topics in our 5-layer Pachinko model.
The root topic is not part of this equation since all words are part of it, so the probability does not need to be calculated.
$Z_{-w}$ is the word topic assignment, for all other words except the one that is being updated.
$n_x^d$ is the number of times topic $t_x$ occurs in document $d$ according to $Z_{-w}$. 
The $n_{xy}^d$ describes how many times topic $t_y$ is sampled from its parent $t_x$ within document $d$ according to $Z_{-w}$.
$n_x$ is the number of times topic $t_x$ occurs in the corpus according to $Z_{-w}$, and $n_{xw}$ is the number of times a word $w$ is in $t_x$ according to $Z_{-w}$.

\input{figures/dag_structure}

\input{figures/pachinko}

\subsubsection{Modification to Pachinko Allocation}\label{subsec:mod_pachinko}
\vejleder[inline]{do you have a comparison in section 5}
Without modification, \gls{pam} will find topics with the same structure as our taxonomy, but the topics will not actually be based on the taxonomy.
However, only $25\%$ of the documents in our dataset have an observed taxonomy.
To account for this, we lock taxonomy for the words in the observed documents to be in the corresponding topics within the pachinko \gls{dag} instead of continuously sampling them using Gibbs sampling.
This creates a constant context for the taxonomy topics, which the documents with unobserved taxonomies will be fitted around.

Some of the documents have multiple taxonomies.
For these documents, one of the taxonomies is chosen randomly for each word in the document.

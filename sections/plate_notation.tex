\section{Plate Notation}
\Gls{lda} infers latent structures in a document set. 
To do this it imagines a generative process that was used to create the document set based on topics, and then reverse-engineer this process to infer the topics.

\subsection{Standard \gls{lda}}
The \gls{lda} by \citet{blei2003latent} imagines the following generative process:
$M$ is the number of documents in the corpus, $N_d$ is number of words in document $d$, and $K$ is the number of topics.
Documents are represented as distributions of topics and topics are distributions over words.
For each document $d \in \{1,\dots, M\}$ a document-topic distribution $\theta_d$ is sampled from a dirichlet distribution parameterized by $\alpha$.
Likewise for each topic $k \in \{1,\dots, K\}$ a topic-word distribution $\beta_k$ is sampled from a dirichlet distribution parameterized by $\eta$.
For each word position $n \in \{1, \dots, N_d\}$ in each document $d$, a topic $z_{d,n}$ is sampled from $\theta_d$ and then a word $w_{d,n}$ is sampled from $\beta_{z_{d,n}}$.
The plate notation for \gls{lda} can be seen in \autoref{fig:standard_lda}.

\input{figures/lda_plate}

\subsection{Author-Topic \gls{lda} and Category-Topic \gls{lda}}
We model both of the metadata fields 'Author' and 'Category' similarly to the model presented by \citet{author_topic_2012}.
In this model, there are no document-topic distributions $\theta$.
Instead, each author and category has its own topic distribution.
This is based on the assumption that authors prefer to write about specific topics, and that categories of the articles were chosen based on the content of the finished article or that local newspapers have their own unique topic preferences.

Our topic models are \glspl{um}, meaning that topics are only word distributions and are chosen based on the data of the documents, rather than \glspl{dm} where topics are fitted to have an influence on both word distributions and other metadata.

\citet{author_topic_2012} describe the author-topic model where for each document $d$, they assign a vector of authors $a_d$ from a set of authors $A$, and for each word draw an author $x$ from this vector.
However, for our category-topic model and our own author-topic model, each document $d$ is associated with one category $c_d$ from a set of categories $C$ and one author $a_d$ from a set of authors $A$, rather than a vector.
This is due to our dataset never having more than one author or category for each document.
Unless otherwise states, future references are to our implementation of this model, rather than the model presented by \citet{author_topic_2012}.
The plate notation for our category and author \gls{lda} models can be seen in \autoref{fig:metadata_lda}.

\begin{figure*}[ht]
	\centering
	\begin{subfigure}{0.3\textwidth}
		\centering
		\input{figures/category_plate}
		\caption{Category \gls{lda}.}
		\label{fig:category_lda}
	\end{subfigure}
	\hspace{5em}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\input{figures/author_plate}
		\caption{Author \gls{lda}.}
		\label{fig:author_lda}
	\end{subfigure}
	\caption{Plate notation for the metadata \gls{lda} models.}
	\label{fig:metadata_lda}
\end{figure*}

\subsection{Pachinko}
\todo[inline]{Description and plate notation}

\todo[inline]{consider the plate notation of combination models}

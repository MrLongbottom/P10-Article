\section{Topic Models}\label{sec:plate_notation}
In this section, topic models that are explored in the experiment are detailed.
This covers the standard \gls{lda} from \citet{blei2003latent}, our category and author metadata models, which build on the concept of \gls{lda}, and our taxonomy metadata model, which uses the \acrlong{pam}.

\subsection{Standard \gls{lda}}
The purpose of \gls{lda}, and topic models in general, is to create a tool for exploring collections of text.
Topic models do this by uncovering the underlying semantic structure of a text collection by using hierarchical Bayesian models.
\Gls{lda} uncovers this semantic structure by discovering patterns of word use in documents and finding topics based on these~\cite{blei2009topic}.

The standard \gls{lda} by \citet{blei2003latent} imagines the following generative process:
$D$ is the number of documents in the corpus, $N_d$ is the number of words in document $d$, $V$ is the size of the vocabulary, and $K$ is the number of topics.
Topics are represented as distributions over words and documents are represented as distributions of topics.
\Gls{lda} assumes that the topics are shared across the corpus, while the document-topic distributions are unique for each document.

For each topic $k \in \{1,\dots, K\}$ a topic-word distribution $\beta_k$ is sampled from a V-dimensional Dirichlet distribution parameterized by $\eta$.
That is, K topics $\beta_{1:k}$ are sampled, each being a distribution over the vocabulary, written as: $\beta_k \sim Dirichlet(\eta)$.
Likewise, for each document $d \in \{1,\dots, D\}$ a document-topic distribution $\theta_d$ is sampled from a K-dimensional Dirichlet distribution parameterized by $\alpha$.
For each word $n \in \{1, \dots, N_d\}$ in each document $d$, a topic $z_{d,n}$ is sampled from a K-multinomial distribution $\theta_d$, and then a word $w_{d,n}$ is sampled from a V-multinomial distribution $\beta_{z_{d,n}}$.
The generative process for each document is seen in these steps:

\vspace{\topsep}
\begin{enumerate}
	\item Draw topic proportion $\theta_d \sim Dirichlet(\alpha)$
	\item For each word $n$ in the document:
	\begin{enumerate}
		\item Draw topic assignment $z_{d,n} \sim Mult(\theta_d)$
		\item Draw word $w_{d,n} \sim Mult(\beta_{z_{d,n}})$
	\end{enumerate}
\end{enumerate}
\vspace{\topsep}

The generative process for topics and documents, generates a list of $K$ topics and $D$ documents that can be used as a $K \times V$ matrix of topic-word distributions and a $D \times K$ matrix of document-topic distributions, respectively.
The plate notation for \gls{lda} can be seen in \autoref{fig:standard_lda}.

\input{figures/lda_plate}

After training the \gls{lda} model, there are multiple possibilities for exploring the corpus using the posterior distributions of the hidden random variables.
One possibility is to visualize the posterior topics of the model, e.g., by sorting $\beta_k$ according to the highest probabilities.
It is also possible to visualize the documents by, e.g., sorting by the highest topic proportions in $\theta_d$.
Another possibility of exploration is finding similar documents by using a distribution distance function on the topic proportions $\theta_d$ between documents~\cite{blei2009topic}.

\subsection{Author-Topic \gls{lda} and Category-Topic \gls{lda}}
We model both of the metadata fields 'Author' and 'Category' similarly to the model presented by \citet{author_topic_2012}.
In this model, there are no document-topic distributions $\theta$.
Instead, each author and category has its own topic distribution.
This is based on the assumption that authors prefer to write about specific topics, and that categories of the articles were chosen based on the content of the finished article or that local newspapers have their own unique topic preferences.

Our topic models are \glspl{um}, meaning that topics are only word distributions and are chosen based on the data of the documents, rather than \glspl{dm} where topics are fitted to have an influence on both word distributions and other metadata.

\citet{author_topic_2012} describe the author-topic model where for each document $d$, they assign a vector of authors $a_d$ from a set of authors $A$, and for each word draw an author $x$ from this vector.
However, for our category-topic model and our own author-topic model, each document $d$ is associated with one category $c_d$ from a set of categories $C$ and one author $a_d$ from a set of authors $A$, rather than a vector.
This is due to our dataset never having more than one author or category for each document.
Unless otherwise stated, future mentions of the author-topic model are to our implementation of this model, rather than the model presented by \citet{author_topic_2012}.
The plate notation for our category and author \gls{lda} models can be seen in \autoref{fig:metadata_lda}.

\begin{figure*}[ht]
	\centering
	\begin{subfigure}{0.3\textwidth}
		\centering
		\input{figures/category_plate}
		\caption{Category \gls{lda}.}
		\label{fig:category_lda}
	\end{subfigure}
	\hspace{5em}
	\begin{subfigure}{0.3\textwidth}
		\centering
		\input{figures/author_plate}
		\caption{Author \gls{lda}.}
		\label{fig:author_lda}
	\end{subfigure}
	\caption{Plate notation for the metadata \gls{lda} models.}
	\label{fig:metadata_lda}
\end{figure*}

\subsection{Pachinko}
\todo[inline]{Description and plate notation}

\todo[inline]{consider the plate notation of combination models}

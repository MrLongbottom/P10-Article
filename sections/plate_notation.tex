\section{Topic Models}\label{sec:plate_notation}
In this section, topic models that are explored in the experiment are detailed.
This covers the standard \gls{lda} from \citet{blei2003latent}, our category and author metadata models, which build on the concept of \gls{lda}, and our taxonomy metadata model, which uses the \acrlong{pam}.

\subsection{Standard \gls{lda}}
The purpose of \gls{lda}, and topic models in general, is to create a tool for exploring collections of text.
Topic models do this by uncovering the underlying semantic structure of a text collection by using hierarchical Bayesian models.
\Gls{lda} uncovers this semantic structure by discovering patterns of word use in documents and finding topics based on these~\cite{blei2009topic}.

The standard \gls{lda} by \citet{blei2003latent} can be described by the following generative process, which is the way the model assumes the documents arose:
$D$ is the number of documents in the corpus, $N_d$ is the number of words in document $d$, $V$ is the size of the vocabulary, and $K$ is the number of topics.
Topics are represented as distributions over words and documents are represented as distributions of topics.
\Gls{lda} assumes that the topics are shared across the corpus, while the document-topic distributions are unique for each document.
For each topic $k \in \{1,\dots, K\}$ a topic-word distribution $\beta_k$ is sampled from a V-dimensional Dirichlet distribution parameterized by $\eta$.
That is, K topics $\beta_{1:k}$ are sampled, each being a distribution over the vocabulary, written as: $\beta_k \sim Dirichlet(\eta)$.
Likewise, for each document $d \in \{1,\dots, D\}$ a document-topic distribution $\theta_d$ is sampled from a K-dimensional Dirichlet distribution parameterized by $\alpha$.
For each word $n \in \{1, \dots, N_d\}$ in each document $d$, a topic $z_{d,n}$ is sampled from a K-multinomial distribution $\theta_d$, and then a word $w_{d,n}$ is sampled from a V-multinomial distribution $\beta_{z_{d,n}}$.
The generative process for each document is seen in these steps:

\vspace{\topsep}
\begin{enumerate}
	\item Draw topic proportion $\theta_d \sim Dirichlet(\alpha)$
	\item For each word $n$ in the document:
	\begin{enumerate}
		\item Draw topic assignment $z_{d,n} \sim Mult(\theta_d)$
		\item Draw word $w_{d,n} \sim Mult(\beta_{z_{d,n}})$
	\end{enumerate}
\end{enumerate}
\vspace{\topsep}

The generative process for topics and documents, generates a list of $K$ topics and $D$ documents that can be used as a $K \times V$ matrix of topic-word distributions and a $D \times K$ matrix of document-topic distributions, respectively.
The plate notation for \gls{lda} can be seen in \autoref{fig:standard_lda}.

\input{figures/lda_plate}

After training the \gls{lda} model, there are multiple possibilities for exploring the corpus using the posterior distributions of the hidden random variables.
One possibility is to visualize the posterior topics of the model, e.g., by sorting $\beta_k$ according to the highest probabilities.
It is also possible to visualize the documents by, e.g., sorting by the highest topic proportions in $\theta_d$.
Another possibility of exploration is finding similar documents by using a distribution distance function on the topic proportions $\theta_d$ between documents~\cite{blei2009topic}.

\subsection{Author-Topic \gls{lda} and Category-Topic \gls{lda}}
We model both of the metadata fields 'Author' and 'Category' similarly to the model presented by \citet{author_topic_2012}.
In this model, there are no document-topic distributions $\theta$.
Instead, each author and category has its own topic distribution.
This is based on the assumption that authors prefer to write about specific topics, and that categories of the articles were chosen based on the content of the finished article or that local newspapers have their own unique topic preferences.

Our topic models are \glspl{um}, meaning that topics are only word distributions and are chosen based on the data of the documents, rather than \glspl{dm} where topics are fitted to have an influence on both word distributions and other metadata.

\citet{author_topic_2012} describe the author-topic model where for each document $d$, they assign a vector of authors $a_d$ from a set of authors $A$, and for each word draw an author $x$ from this vector.
However, for our category-topic model and our own author-topic model, each document $d$ is associated with one category $c_d$ from a set of categories $C$ and one author $a_d$ from a set of authors $A$, rather than a vector.
This is due to our dataset never having more than one author or category for each document.
Unless otherwise stated, future mentions of the author-topic model are to our implementation of this model, rather than the model presented by \citet{author_topic_2012}.
The plate notation for our category and author \gls{lda} models can be seen in \autoref{fig:metadata_lda}.

\begin{figure*}[ht]
	\centering
	\begin{subfigure}{0.25\textwidth}
		\centering
		\resizebox{\textwidth}{!}{%
		\input{figures/category_plate}
		}
		\caption{Category \gls{lda}.}
		\label{fig:category_lda}
	\end{subfigure}
	\hspace{2em}
	\begin{subfigure}{0.25\textwidth}
		\centering
		\resizebox{\textwidth}{!}{%
			\input{figures/author_plate}
		}
		\caption{Author \gls{lda}.}
		\label{fig:author_lda}
	\end{subfigure}
	\hspace{2em}
	\begin{subfigure}{0.25\textwidth}
		\centering
		\resizebox{\textwidth}{!}{%
			\input{figures/author_category_plate}
		}
		\caption{Author-Category \gls{lda}.}
		\label{fig:author_category_lda}
\end{subfigure}
	\caption{Plate notation for the metadata \gls{lda} models.}
	\label{fig:metadata_lda}
\end{figure*}

\subsection{Pachinko Allocation}
In order to handle the hierarchical structure of the taxonomy metadata field, we will be using a hierarchical topic model, namely the \acrlong{pam} from \citet{li2006pachinko}.
Pachinko allocation generalizes \gls{lda}, to be able to operate on any \gls{dag} structure.
\vejleder[inline]{struktur over hvad?}

The \gls{dag} structure \vejleder[inline]{illustrer gerne} represents topics which in pachinko allocation are distributions over words and/or other topics further down in the \gls{dag} structure, rather than a distribution only over words as with \gls{lda}.
The Pachinko model is set up in a \gls{dag} structure with the first layer being the root node, a topic which all documents are part of, and the bottom layer consisting of leaf nodes, which are words.
The middle layers will represent the taxonomy entries in our dataset.

For simplicity, we will be using a five-level pachinko tree structure, with each node in a layer having an edge to each node in the next layer.
We therefore also limit the taxonomy dataset to only the first $3$ taxonomy entries for each document, to fit with our \gls{dag} structure.
This \gls{dag} structure is visualized in \autoref{fig:pachinko_dag}.

For each word, a chain of topics is sampled by sampling one topic from each layer of the structure, and finally, a word is chosen based on the last topic.
More specifically, for each topic $Z_{wl}$ starting with the root topic $Z_{w1}$, we sample a topic from the next layer $Z_{w(l+1)}$ from that node's topic distribution $\theta_l^{(d)}$ based on observed data from that document.
In \autoref{eq:pachinko_gibbs}, the joint probability for our model is shown.
This calculation corresponds to the generative process in \autoref{fig:pachinko}, where each level of the Pachinko is sampled in the inner plate.

\begin{equation}\label{eq:pachinko_gibbs}
	\begin{split}
		P(Z_{w2} = t_a, Z_{w3} = t_b, Z_{w4} = t_c | \textbf{D}, z_{-w}, \alpha, \beta) \propto \\
		\frac{n_{1a}^d + \alpha_{1a}}{n_1^d + \sum_{a'} \alpha_{1a'}} \times
		\frac{n_{ab}^d + \alpha_{ab}}{n_a^d + \sum_{b'} \alpha_{ab'}}  \times 
		\frac{n_{bc}^d + \alpha_{bc}}{n_{b}^d + \sum_{c'} \alpha_{bc'}} \\ \times 
		\frac{n_{cw}^d + \beta_{w}}{n_{c} + \sum_{m} \beta_{m}} 
	\end{split}
\end{equation}
As in \citet{li2006pachinko} $t_1$ \vejleder[inline]{Hvorfor er t1 ikke i eq:1 og hvor kommer eq:1 fra} is the root topic and $Z_{w2}$, $Z_{w3}$, $Z_{w4}$ are topic assignments for the three middle layers of topics in our 5-layer Pachinko model.
$Z_{-w}$ is the word topic assignment, for all other words except the one that is being updated.
Without looking at the current object of iteration, $n_x^d$ is the number of times topic $t_x$ occurs in document $d$. 
The $n_{xy}^d$ describes how many times topic $t_y$ is sampled from its parent $t_x$ within document $d$.
$n_x$ is the number of times subtopic $t_x$ occurs in the corpus and $n_{xw}$ is the number of times a word $w$ is in $t_x$.

\todo{describe why we use 5 layers.}
\todo[inline]{add example (steder -> denmark -> aalborg)}

\input{figures/dag_structure}

\input{figures/pachinko}

\subsubsection{Modification to Pachinko Allocation}
Without modification, \gls{pam} will find topics with the same structure as our taxonomy, but the topics will not actually be based on the taxonomy, since only $25\%$ of the documents have an observed taxonomy.
To account for this, we lock taxonomy for the words in the observed documents to be in the corresponding topics within the pachinko \gls{dag} instead of continuously sampling them using Gibbs sampling.
This creates a constant context for the taxonomy topics, which the documents with unobserved taxonomies will be fitted around.

Some of the observed documents have multiple taxonomies.
For these documents, one of the taxonomies is chosen randomly for each word in the document.

\subsection{Combinations}\label{subsec:combinations}
In the Author-Category combination model, we simply multiply the topic distributions $\theta_a$ and $\theta_c$ together.
Then, when sampling for a new word topic, we normalize the product of the multiplication to sum to 1, which also makes it a distribution.
This makes both of the learned meta distributions influence the sampled word topics.
The plate notation for the Author-Category model can be seen in \autoref{fig:author_category_lda}.

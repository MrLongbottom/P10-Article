\section{Related Work}

\citet{mimno2008topic} describes two general topic model patterns, called ''upstream`` and ''downstream`` topic models.
These patterns describe how metadata is incorporated in topic models.
In the downstream pattern, metadata is incorporated in the model by having the topic model generate both the words and the metadata simultaneously.
Here each topic has a distribution over words, as well as a distribution over metadata values.
In the upstream pattern, the topic model is conditioned on metadata elements by representing document-topic distributions as element specific distributions.
This way the model learns assignments of the words in each document to the metadata.

\citet{author_topic_2012} have developed a \gls{lda} model called the Author-Topic model, which incorporates authorship information in the \gls{lda} model.
Specifically, each document has a vector of authors, where for each word an author is drawn from this vector.
The author is then used in combination with an author-topic distribution to draw a specific topic that this author writes about.
The words from the topic-word distribution can be generated based on this topic.
The purpose of using authorship information in this way, is to show patterns in which topics an author usually writes about, and be able to explore how related authors are in what they write about.
\citeauthor{author_topic_2012} also show that the combination of authorship and \gls{lda} yield more coherent topics.


\citet{MetaLDA2017} presents a resent model which can incorporate both meta data information and word embeddings within a topic model.
Since the field of incorporating word embedding within generative topic model have gained popularity\cite{dieng2020topic}, the authors of \cite{MetaLDA2017} show how to use this information for a variety of different datasets.
They also compare against a list of other models that take either meta data information or word embeddings into account when doing inference.


\citet{tea_leaves} is a well-known paper within the topic modelling community, which present different methods for evaluating probabilistic topic models. 
The important aspect they show is that a good held-out likelihood, normally called perplexity, score infers less semantically meaningful topics.
They also show different human judgement methods, which can be used to evaluate topic model but they do require a large amount of time to evaluate if a large corpus is used.
\citet{topic_coherence_2015} introduce a new measures for evaluating topic models where some of them use the co-occurrence of words within topic to measure how coherent the topics are. 
They use different metrics for comparing the coherence of a topic, like co-occurrence of words and condition probabilities of words.
In order to verify that these metrics work, they conduct a large user study in conjunction with these metric evaluation.
They find that the $C_v$ metric scores the best across they experiments. 


\citet{li2006pachinko} presents a \gls{dag} structured topic model where topics are in a hierarchical structure, which allows it to find two types of topics, namely super-topics and sub-topics. 


\citet{card2017neural} describe how to incorporate metadata information within a neural network to find topics using variational inference methods.

\todo[inline]{Maybe some text that leads to the next section. What concepts do we use from the related work.}
\subsection{Stemming the dataset}\label{sec:stemming}
In this project we have done minimal preprocessing of the dataset, because in a previous project, a more aggressive preprocessing step turned out to have a negative effect on the models.
This made us completely avoid trying to include a stemming process, even though this was only a smaller part of this previous preprocessing step.
Though, after further consideration, it is worth looking into what effect adding just stemming to our current preprocessing, described in \autoref{sec:dataset}, would have on the standard \gls{lda} model.

With stemming included, the number of unique words goes down to $60,651$ from the previous $69,192$ unique words.
This means that 8541 unique words have been stemmed down to their root forms.
Even though much fewer unique words are in our dataset, which can have an effect on which articles are removed, the dataset only goes from $139,064$ articles to $139,060$, which is a removal of 4 articles.


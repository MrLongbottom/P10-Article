\subsection{Topic coherence}\label{app:topic_coherence}
The equations, explanations, and values of the hyperparameters in this section are based on~\citet{Syed2017coherence} and the \textit{gensim} python package\footnote{\url{https://radimrehurek.com/gensim/}}.
Calculating topic coherence requires the following steps:

\begin{enumerate}
\item Topic-word segmentation into word set pairs
\item Word and word pair probability calculation
\item Word set confirmation measure
\item Aggregation of confirmation measures
\end{enumerate}

For segmentation, a set of word pairs $S$ is created, which pairs each word in the top-N most probable words $W$ in a specific topic $t$ with all other words in $W$.
$S$ is defined by \autoref{eq:set_of_word_pairs}.

\begin{equation}\label{eq:set_of_word_pairs}
	S = \{(W', W*)|W' = \{w_i\};w_i \in W;W* = W\}
\end{equation}

Before calculating word probabilities, a sliding window of size $s$ where $s =110$ is used to create a set of subdocuments $D_s$ over the document set $D$, by creating one subdocument for every window of size $s$ by sliding over each document at a rate of one word per step.
So document $d = \{w_1, w_2, \dots, w_l\}$, would be converted into $d_1=\{w_1, w_2, \dots, w_s\}, d_2=\{w_2, w_3, \dots, w_{s+1}\}, \dots, d_{l-s}=\{w_{l-s}, w_{1+l-s}, \dots, w_l\}$.
If $l$ is smaller than $s$ only a single subdocument is made.
This also means that the value of $s$ has impact on how much influence each document has on the metric.
We choose not to change the sliding window size $s$ to be able to compare against other papers, but this is a hyperparameter, which could provide better results for our dataset if changed.
These subdocuments are used rather than the normal documents to capture some degree of word proximity.
Word probabilities are calculated based on how many documents, within the document set $D_s$, they occur in.
$p(w_i)$ is the number of subdocuments in which the word $w_i$ occurs divided by $|D_s|$.
$p(w_i, w_j)$ is the number of subdocuments in which both words occur divided by $|D_s|$. 

As part of the word set confirmation measure we create a \gls{npmi} matrix of size $|W|\times|W|$, with one entry per word pair combination in $W$.
\begin{equation}\label{eq:coherence_2}
	\text{NPMI}(w_i,w_j) =  \frac{\log\frac{p(w_i,w_j) + \epsilon}{p(w_i)\cdot p(w_j)}}{-log(p(w_i,w_j) + \epsilon)}
\end{equation}
\noindent where $\epsilon$ is a low number ($10^{-12}$) used to avoid $log(0)$.
The \gls{npmi} matrix describes how much each word in the topic co-occurs with the other words.
Each value is between $-1$ and $1$, with $-1$ meaning that the words never occur together and 1 meaning that they only occur together.

After having calculated the \gls{npmi} matrix, we construct context vectors for both elements $W'$ and $W*$ in each word-pair $S_i$, by summing over the rows of the \gls{npmi} matrix.
This summation describes how much each top word co-occurs with the other words in $W$.
\begin{equation}\label{eq:coherence_1}
	\overrightarrow{v}(W') = \left\{ \sum_{w_i \in W'} \text{NPMI}(w_i, w_j)^{\gamma} \right\}_{j=1,\dots,|W|}
\end{equation}
\noindent where $\gamma$ can be used to further prioritize higher values.
For this paper we use $\gamma = 1$, as recommended by \citet{Syed2017coherence}.

We now have a pair of context vectors for each word pair $S_i$ and we want to know how different these vectors are.
This is calculated using cosine similarity as a confirmation measure.
\begin{equation}\label{eq:coherence_3}
	\phi_{S_i}(\overrightarrow{u}, \overrightarrow{w}) = \frac
	{\sum_{i = 1}^{|W|} u_i \cdot w_i}
	{\|\overrightarrow{u}\|_2 \cdot \|\overrightarrow{w}\|_2}
\end{equation}

\noindent where $\overrightarrow{u}$ is the context vector $\overrightarrow{v}(W')$, and $\overrightarrow{w}$ is the context vector $\overrightarrow{v}(W^*)$.

Lastly, the confirmation measures are aggregated using the arithmetic mean, to achieve the coherence value of topic $t$.

\begin{equation}\label{eq:coherence_4}
	C_v = \frac{1}{|S|}\sum_{i=1}^{|S|}\phi_{S_i}
\end{equation}

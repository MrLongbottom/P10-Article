\subsection{Gibbs sampling}\label{sec:appendix_gibbs}
The Gibbs Sampling algorithm consists of two procedures: Random Initialization and Gibbs Sampling.
In the following sections, we explain how these procedures have been implemented.

\subsubsection{Random Initialization}
Before a Gibbs sampling algorithm can run, every word needs to be assigned a random topic.
To do this, we iterate over each word within each document and assign a random topic to it.
This procedure is shown in \autoref{lst:randomInit}.
\begin{lstlisting}[language=Python, caption=Random Initialization for the Gibbs sampler.,label={lst:randomInit}, float, floatplacement=H]
import numpy as np

def rand_initialize(documents: List[np.ndarray]):
	wt_assignment = []
	for doc in documents:
		curr_doc = []
		for word in doc:
			# Construct the topic distribution
			pz = _conditional_distribution()
			
			# Draw a new topic and assign it
			t = np.random.multinomial(1, pz).argmax()
			curr_doc.append(t)
			
			# Increase the topic counts
			increase_count()
		wt_assignment.append(curr_doc)
	return wt_assignment
\end{lstlisting}
Various parameters are left out to simplify the code listing of our initialization method.
The \emph{\_conditional\_distribution()} function creates a distribution of topics based on the current word.
Standard \gls{lda} is based on \autoref{eq:gibbs_eq}~\cite{author_topic_2012} and the author-topic and category-topic models are based on \autoref{eq:author_gibbs_eq} and \autoref{eq:category_gibbs_eq}, respectively.
\begin{equation}\label{eq:gibbs_eq}
	\begin{split}
		P(z_i = k|w_i = m, \boldsymbol{z}_{-i}, \boldsymbol{w}_{-i}) \propto 
		\underbrace{\frac{C^{DT}_{dk} + \alpha}{\sum_{k'} C^{DT}_{dk'} + T\alpha}}_{Doc-Topic}
		\underbrace{\frac{C^{WT}_{mk} + \eta}{\sum_{m'} C^{WT}_{m'k} + V\eta}}_{Topic-Word}
	\end{split}
\end{equation}
\begin{equation}\label{eq:author_gibbs_eq}
	\begin{split}
		P(z_i = k|w_i = m, \boldsymbol{z}_{-i}, \boldsymbol{w}_{-i}, a_d) \propto 
		\underbrace{\frac{C^{AT}_{ak} + \alpha}{\sum_{k'} C^{AT}_{ak'} + T\alpha}}_{Author-Topic}
		\underbrace{\frac{C^{WT}_{mk} + \eta}{\sum_{m'} C^{WT}_{m'k} + V\eta}}_{Topic-Word}
	\end{split}
\end{equation}
\begin{equation}\label{eq:category_gibbs_eq}
	\begin{split}
		P(z_i = k|w_i = m, \boldsymbol{z}_{-i}, \boldsymbol{w}_{-i}, c_d) \propto 
		\underbrace{\frac{C^{CT}_{ck} + \alpha}{\sum_{k'} C^{CT}_{ck'} + T\alpha}}_{Category-Topic}
		\underbrace{\frac{C^{WT}_{mk} + \eta}{\sum_{m'} C^{WT}_{m'k} + V\eta}}_{Topic-Word}
	\end{split}
\end{equation}
where $C^{DT}_{dk}$ is the number of times document $d$ uses topic $k$ and $C^{WT}_{mk}$ is the number of times topic $k$ uses word $m$.
$C^{AT}_{ak}$ and $C^{CT}_{ck}$ is the number of times author $a$ and category $c$ use topic $k$, respectively.
$\boldsymbol{z}_{-i}$ represents the topic assignments where the current instance is disregarded.
$\alpha$ and $\eta$ are the Dirichlet parameters for the document-topic and topic-word distribution, respectively.
The first fraction describes how likely topic $t$ appearing in document $d$ is, and the second fraction describes which words are most probable in topic $t$.
Following the code in \autoref{lst:randomInit}, as we initialize, the words get a higher probability of clustering together, since we increase the counts every time at line $16$.
In the author-topic and category-topic models, a metadata distribution is used instead of the document-topic distribution.
This can be seen in \autoref{eq:author_gibbs_eq} and \autoref{eq:category_gibbs_eq}



\subsubsection{Gibbs Sampling}
We can start investigating the Gibbs sampling method itself, where we iterate over each word in every document and draw a new topic based on the given topic distribution.
As in \autoref{lst:randomInit}, the code has been simplified.
\begin{lstlisting}[language=Python, caption=The Gibbs Sampling Method.,label={lst:gibbsSampling}, float, floatplacement=H]
import numpy as np

def gibbs_sampling(documents: List[np.ndarray],
	   doc_topic_dist: np.ndarray,
	   doc_topic_count: np.ndarray,
	   topic_word_dist: np.ndarray,
	   topic_word_count: np.ndarray,
	   wt_assignment: List[List[int]]):

	for d_index, doc in documents:
		for w_index, word in enumerate(doc):
			# Get the topic of the current word
			topic = wt_assignment[d_index][w_index]
			
			# Decrease the topic count
			decrease_count()
			
			# Sample a new topic
			pz = _conditional_distribution()
			topic = np.random.multinomial(1, pz).argmax()
			
			# Assign topic to the current word
			wt_assignment[d_index][w_index] = topic
			
			# And increase the topic count
			increase_count()
\end{lstlisting}
The Gibbs sampling method is very similar to the random initialization method in \autoref{lst:randomInit}, but with a few additions. 
Now we introduce the \emph{decrease\_count()} which decreases the topic count for both words and documents, and  \emph{increase\_count()} which increases them.
This is done because new samples need to be calculated based on all other word topic assignments (not including the current word).
The sampling is explained in \autoref{lst:gibbsSampling}.
On line 13, we get the current topic for the given word and on line 23 we assign a newly drawn topic to that word.

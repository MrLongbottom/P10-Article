\subsection{Pachinko implementation}
We have implemented a \acrfull{pam} algorithm able to support any \gls{dag} structure, where each layer only has edges to all the nodes in the next layer, as with the 'Four-Level \gls{pam}' presented by \citet{li2006pachinko}.

We use Gibbs sampling for performing inference.
For each word, a chain of topics is sampled by calculating the probability of all combinations of topics and making a weighted sample.
The probability of each topic combination is calculated using the joint probability of the topics, as presented in \autoref{eq:pachinko_gibbs}. This equation is for the 'Five-Level \gls{pam}' that we use in the paper.

\begin{equation}\label{eq:pachinko_gibbs}
	\begin{split}
		P(Z_{w2} = t_a, Z_{w3} = t_b, Z_{w4} = t_c | \textbf{D}, z_{-w}, \alpha, \beta) \propto
		\underbrace{\frac{n_{1a}^d + \alpha_{1a}}{n_1^d + \sum_{a'} \alpha_{1a'}}}_{\text{Root $\rightarrow$ Tax.1}} \times
		\underbrace{\frac{n_{ab}^d + \alpha_{ab}}{n_a^d + \sum_{b'} \alpha_{ab'}}}_{\text{Tax.1 $\rightarrow$ Tax.2}}  \times 
		\underbrace{\frac{n_{bc}^d + \alpha_{bc}}{n_{b}^d + \sum_{c'} \alpha_{bc'}}}_{\text{Tax.2 $\rightarrow$ Topics}} \times 
		\underbrace{\frac{n_{cw} + \eta_{w}}{n_{c} + \sum_{m} \eta_{m}}}_{\text{Topics $\rightarrow$ Words}} 
	\end{split}
\end{equation}
As in \citet{li2006pachinko}, $Z_{w2}$, $Z_{w3}$, and $Z_{w4}$ are topic assignments for the three middle layers of topics in our Five-Level \gls{pam}.
The root topic is not part of this equation since all words are part of it, so the probability does not need to be calculated.
$Z_{-w}$ is the word topic assignment, for all other words except the one that is being updated.
$n_x^d$ is the number of times topic $t_x$ occurs in document $d$ according to $Z_{-w}$. 
The $n_{xy}^d$ describes how many times topic $t_y$ is sampled from its parent $t_x$ within document $d$ according to $Z_{-w}$.
$n_x$ is the number of times topic $t_x$ occurs in the corpus according to $Z_{-w}$, and $n_{xw}$ is the number of times a word $w$ is in $t_x$ according to $Z_{-w}$.

However, the \gls{pam} framework can support any number of layers using this structure.
In order to do this, we must generalize the process of Gibbs sampling for pachinko using 'level' \gls{dag} structures.
Firstly, before the Gibbs sampling begins a one-time random initialization is made, where each word in each document is randomly assigned to a chain of topics (one for each layer).
In our \gls{pam}, some topic layers represent taxonomy layers, since some documents in the dataset already have a topic entry.
These documents are assigned the topics corresponding to their taxonomy entries, and the rest of the taxonomy chain is then randomly generated if it is not already complete.

The Gibbs sampling for \gls{pam} consists of the following steps for each word in each document:

\begin{enumerate}
	\item Decrease count
	\item Calculate layer combinations
	\item Multiply layer combinations
	\item Weighted sample
	\item Increase count
\end{enumerate}

Following is an overview of each of the steps.
Firstly, the current word is removed from the counts of how many words are assigned to each topic.
After the count has been decreased, we calculate for each combination of successive layers, the probability of each possible topic combination.
This process is explained further in \autoref{app:calculate_layer_combs}.

Each of these calculations is combined to calculate the final probability of each possible topic combination.
This process is explained in \autoref{app:multiply_layer_combs}.
One topic combination is then sampled, using a weighted sampling based on the probabilities of all topic combinations.
Finally, once a new topic combination has been chosen, the counts of how many words are assigned to each topic are increased accordingly.

In the next section, details about calculating layer combinations and multiplying layer combinations are explained, since these are the main differences between how \gls{lda} and \gls{pam} use Gibbs sampling.
\subsubsection{Calculate layer combinations}\label{app:calculate_layer_combs}
This is done based on observations in \autoref{eq:pachinko_gibbs}.
The equation consist of several fractions equal to the number of layers - 1, with each fraction representing the relationship between two layers.
The last fraction is a little different as it takes word topic assignments for the whole corpus into account, unlike the other fractions which only look at the word topic assignments for the current document.

In order to run efficiently, we calculate all topic combinations at the same time, rather than calculating a specific one as outlined in \autoref{eq:pachinko_gibbs}.
To do this, we operate on vectors and matrices rather than single values.
So for the fraction $\frac{n_{ab}^d + \alpha_{ab}}{n_a^d + \sum_{b'} \alpha_{ab'}}$ from \autoref{eq:pachinko_gibbs}, $n_{ab}^d$ is a matrix which indicates the number of words in document $d$ that has been assigned to each combination of topics in layer $a$ and layer $b$, with one row for each topic in layer $a$ and one column for each topic in layer $b$.
Similarly, $n_a^d$ is a vector showing the number of words in document $d$ assigned to each topic in layer $a$, rather than a single topic.
If some taxonomy entries for the document are already known, the matrices and vectors are sliced to only include the relevant unknown topics.

\subsubsection{Multiplying layer combinations}\label{app:multiply_layer_combs}
Once all the two-layer combinations have been calculated, they have to be combined to find the probability of all topic combinations.
To do so, the layer combinations are multiplied across the dimensions they share.
So for an $A\times B$ matrix and a $B\times C$ matrix, values that share the same $B$ entry are multiplied together to form a three-dimensional $A\times B\times C$ array.
Importantly, the shared dimension is kept, unlike with matrix multiplication.
By keeping all dimensions, the final array has one entry for all possible topic combinations.

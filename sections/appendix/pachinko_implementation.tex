\subsection{Pachinko Implementation}
We have implemented a pachinko algorithm able to support any \gls{dag} structure, where each layer only has edges to all the nodes in the next layer, as with the 'Four-Level \gls{pam} presented' by \citet{li2006pachinko}.
It can therefore support any number of layers using this structure.
In order to do this we must generalize the process of gibbs sampling for pachinko using 'level' \gls{dag} structures.

\begin{equation}\label{eq:pachinko_gibbs}
	\begin{split}
		P(Z_{w2} = t_a, Z_{w3} = t_b, Z_{w4} = t_c | \textbf{D}, z_{-w}, \alpha, \beta) \propto \\
		\frac{n_{1a}^d + \alpha_{1a}}{n_1^d + \sum_{a'} \alpha_{1a'}} \times
		\frac{n_{ab}^d + \alpha_{ab}}{n_a^d + \sum_{b'} \alpha_{ab'}}  \times 
		\frac{n_{bc}^d + \alpha_{bc}}{n_{b}^d + \sum_{c'} \alpha_{bc'}} \\ \times 
		\frac{n_{cw}^d + \beta_{w}}{n_{c} + \sum_{m} \beta_{m}} 
	\end{split}
\end{equation}

Firstly, before the gibbs sampling begins a one time random initilization is made, where each word in each document is randomly assigned to a chain of topics (one for each layer).

In our pachinko model some topuic layers represent taxonomy layers, there are some documents in the dataset that already have a topic entry.
These documents, will be assigned the topics corresponding to their taxonomy entries, and the rest of the taxonomy chain will then be randomly generated if it is not already complete.

The gibbs sampling for \gls{pam}, consists of the following steps for each word in each document:

\begin{enumerate}
	\item Decrease Count
	\item Calculate Layer Combinations
	\item Multiply Layer Combinations
	\item Weighted Sample
	\item Increase Count
\end{enumerate}

Firstly the current word are removed from the counts of how many words are assigned to each topic.
After this for each combination of sucessive layers, the probability of each possible topic cobination is calculated.
This process will be explained further later in \autoref{app:calculate_layer_combs}.
Each of these calculation is combined in order to calculate the final probability of each possible topic combination.
One topic combination is then sampled, using a weighted sampling based on the probabilities of all topic combinations.
Finally, once a new topic combination has been chosen, the counts of how many words are assigned to each topic are increased occordingly.

\subsubsection{Calculate Layer Combinations}\label{app:calculate_layer_combs}
This is done based on observations in \autoref{eq:pachinko_gibbs}.
The equation consist of a number of fractions equal to the number of layers - 1, with each fraction representing the relationship between two layers.
The last fraction is a little different as it takes word topic assignments for the whole corpus into account, unlike the other fractions which only look at the word topic assignments for the current document.

In order to run efficiently, we will be calculating all topic combinations at the same time, rather than calculating a specific one as outlined in \autoref{eq:pachinko_gibbs}.
To do this, we operate on vectors and matrices rather than single values.
So for the fraction $\frac{n_{ab}^d + \alpha_{ab}}{n_a^d + \sum_{b'} \alpha_{ab'}}$ from \autoref{eq:pachinko_gibbs}, $n_{ab}^d$ is a matrix which indicates the number of words in document $d$ that has been assigned to each combination of topics in layer $a$ and layer $b$, with one row for each topic in layer $a$ and one column for each topic in layer $b$.
Similarly $n_a^d$ is a vector showing the number of words in document $d$ assigned to each topic in layer $a$, rather than a single topic.
If some taxonomy entries for the document are already known, the matrices and vectors are sliced to only include the relevant values.

\subsubsection{Multiplying Layer Combinations}
Once all the two-layer combinations have been calculated they have to be combined to find the probability of all topic combinations.
To do so layer combinations are multiplied across the dimensions they share.
So for a $AxB$ matrix and a $BxC$ matrix, values that share the same $B$ entry are multiplied together to form a three dimensional $AxBxC$ array.
Importantly the shared dimension is kept, unlike with matrix multiplication.
By keeping all dimensions, the final array will have one entry for all possible topic combinations.


\subsection{Taxonomy-topic model}\label{sec:taxonomy_analysis}
Finally, we also want to analyze the taxonomy-topic model, especially since this model has the highest topic coherence out of all the tested models.
\autoref{tab:pachinko_selected_topics} shows selected lowest level topics from the the taxonomy-topic model.
As can be seen from the figure most of the topics are quite understandable.
However, there are some topics (e.g., topics 19 and 42) that consist entirely of words that provide little context or semantic meaning.
This indicates that the model has learned to group words that do not belong to any good topics.
This is a good feature that allows the model to apply an extra layer of preprocessing, automatically filtering away irrelevant words into topics.
This feature is also seen in some other topic models, such as in the hierarchical \gls{lda} (hLDA) by \citet{hLDA2004} and the embedded topic model (ETM) by \citet{dieng2020topic}, but the \gls{lda} does not seem to have this feature.

Since this model deals with more topic distributions than the other models, it is worth checking whether it also converges within the first 50 epochs, as with \gls{lda}.
This does seem to be the case, as indicated by \autoref{fig:pachinko_train}.
Here it can be seen that the topic coherence curve has flattened significantly, and thus additional epochs would yield diminishing returns.

\autoref{tab:pachinko_mid_topics} gives an overview of how the taxonomy topics in the third layer of the taxonomy-topic model, are connected to the fourth layer topics that were generated by the model.
Some of these connections make a lot of sense, such as the 'Økonomi' (Economy) taxonomy entry topic which has the three filler topics (i.e., topics which consists entirely of words with little semantic value) within the top 5 most probable topics: 79, 75, and 42, as well as two topics which are about money: 74 and 9.
\autoref{tab:pachinko_selected_topics} shows the top words for each of these topics.
However not all the connections between higher and lower level topics are as understandable as these. 
For example, the 'Kriminalitet' (Crime) taxonomy entry has two filler topics within the 5 most probable topics: 42 and 75, one topic about economy: 60, one topic about politics: 8, and one topic about sports: 86.
\autoref{tab:pachinko_selected_topics} shows the top words for the topics mentioned in this section, and Appendix \autoref{tab:pachinko_topics} shows top words for all topics in the taxnomy-topic model.

Having the layered structure of the \gls{pam} gives many possibilities for recommending new articles to readers.
There is the possibility of exploring the similarity of taxonomies at the same layer and using this to recommend new articles with similar subjects.
For example, if an article is about 'Miljø' (environment) similar taxonomies might be 'Natur' (nature) and possibly 'Etik' (ethics), 'Trafik' (traffic), and 'Energi' (energy).
